{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ann\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Perceptron is one of the simplest ANN architectures, invented in 1957 by Frank\n",
    "Rosenblatt. \n",
    "\n",
    "- It is based on a slightly different artificial neuron called a linear threshold unit (LTU), as lectured.\n",
    "- Its inputs and output are now numbers (instead of binary on/off values) and each input connection is associated with a weight. \n",
    "\n",
    "- The LTU computes a weighted sum of its inputs, then applies a step function to that sum and outputs the result:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn provides a Perceptron class that implements a single LTU network. \n",
    "# It can be used pretty much as you would expect for example, on the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "\n",
    "\n",
    "# you may ignore \"Warning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = per_clf.predict([[2, 0.5]])\n",
    "# Perceptrons do not output a class probability; \n",
    "# rather, they just make predictions based on a hard threshold. \n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You may have recognized that the Perceptron learning algorithm strongly resembles\n",
    "Stochastic Gradient Descent. In fact, Scikit-Learn’s Perceptron class is equivalent to\n",
    "using an SGDClassifier with the following hyperparameters: loss=\"perceptron\" ,\n",
    "learning_rate=\"constant\" , eta0=1 (the learning rate), and penalty=None (no regu‐\n",
    "larization).\n",
    "\n",
    "\n",
    "\n",
    "- Note that contrary to Logistic Regression classifiers, Perceptrons do not output a class\n",
    "probability; rather, they just make predictions based on a hard threshold. This is one\n",
    "of the good reasons to prefer Logistic Regression over Perceptrons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us have a plot for iris classification using Perceptron\n",
    "\n",
    "a = -per_clf.coef_[0][0] / per_clf.coef_[0][1]\n",
    "b = -per_clf.intercept_ / per_clf.coef_[0][1]\n",
    "\n",
    "axes = [0, 5, 0, 2]\n",
    "\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(axes[0], axes[1], 500).reshape(-1, 1),\n",
    "        np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "y_predict = per_clf.predict(X_new)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "\n",
    "plt.plot([axes[0], axes[1]], [a * axes[0] + b, a * axes[1] + b], \"k-\", linewidth=3)\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap, linewidth=5)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.axis(axes)\n",
    "\n",
    "save_fig(\"perceptron_iris_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my lecture, I introduced four Activation Functions:\n",
    "- Step function\n",
    "- Logistic function\n",
    "- The hyperbolic tangent function\n",
    "- The ReLU finction\n",
    "\n",
    "\n",
    "Now, let us plot them for your undersdaning their difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=2, label=\"Step\")\n",
    "plt.plot(z, logit(z), \"g--\", linewidth=2, label=\"Logit\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Activation functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=2, label=\"Step\")\n",
    "plt.plot(0, 0, \"ro\", markersize=5)\n",
    "plt.plot(0, 0, \"rx\", markersize=10)\n",
    "plt.plot(z, derivative(logit, z), \"g--\", linewidth=2, label=\"Logit\")\n",
    "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, derivative(relu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "#plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Derivatives\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "save_fig(\"activation_functions_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNN for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Training Mulptple Layer Perceptrons (MLP) with TensorFlow’s High-Level API **\n",
    "\n",
    "- The simplest way to train an MLP with TensorFlow is to use the high-level API\n",
    "TF.Learn, which is quite similar to Scikit-Learn’s API. \n",
    "\n",
    "\n",
    "- The DNNClassifier class makes it trivial to train a deep neural network with any number of hidden layers, and a softmax output layer to output estimated class probabilities. \n",
    "\n",
    "\n",
    "- For example, the following code trains a DNN for classification with two hidden layers (one with 300 neurons, and the other with 100 neurons) and a softmax output layer with 10 neurons:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using tf.learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the MNIST dataset, which is a set of 70,000 small\n",
    "# images of digits handwritten by high school students and employees of the US Census Bureau. \n",
    "# Each image is labeled with the digit it represents.\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Tensorflow for this classifications\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# RunConfig is optional, you may ignore it or use it.\n",
    "# config = tf.contrib.learn.RunConfig(tf_random_seed=42) \n",
    "\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n",
    "\n",
    "# Select a training model-\"DNNClassifer\" from Tensorflow and specify its parameters,\n",
    "# Create the model.\n",
    "\n",
    "# with Runconfig\n",
    "# dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100], n_classes=10,\n",
    "                                         #feature_columns=feature_cols, config=config)\n",
    "# without Runcongig:\n",
    "\n",
    "dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300,100], n_classes=10,\n",
    "                                         feature_columns=feature_cols)\n",
    "# if TensorFlow version >= 1.1\n",
    "\n",
    "dnn_clf = tf.contrib.learn.SKCompat(dnn_clf) \n",
    "dnn_clf.fit(X_train, y_train, batch_size=50, steps=40000)\n",
    "\n",
    "# It takes a long time for training process, please be patient. Wait until finished.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred['classes'])\n",
    "\n",
    "# it also takes some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "y_pred_proba = y_pred['probabilities']\n",
    "log_loss(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Training an Mulptple Layer Perceptrons (MLP) with TensorFlow’s low-Level **\n",
    "\n",
    "If you want more control over the architecture of the network, you may prefer to use\n",
    "TensorFlow’s lower-level Python API.  It means that you may build a neural network by yourself,\n",
    "rather than by using high-level API, such as using DNNClassifier model.\n",
    "\n",
    "In this section we, will build the same model as before using this API, and we will implement Mini-batch Gradient Descent to train it on the MNIST dataset. \n",
    "\n",
    "The first step is the construction phase, building the TensorFlow graph. The second step is the execution\n",
    "phase, where you actually run the graph to train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using plain TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction Phase:\n",
    "# First we need to import the tensorflow library. Then we must specify the\n",
    "# number of inputs and outputs of a neural network, \n",
    "# and set the number of hidden neurons in each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, you can use placeholder nodes to represent thetraining data and targets. \n",
    "# The shape of X is only partially defined. \n",
    "# We know that it will be a 2D tensor (i.e., a matrix), \n",
    "# with instances along the first dimension and features along the second dimension, \n",
    "# and we know that the number of features is going to be\n",
    "# 28 x 28 (one feature per pixel), but we don’t know yet how many instances each train‐\n",
    "# ing batch will contain. So the shape of X is (None, n_inputs) . \n",
    "# Similarly, we know that y will be a 1D tensor with one entry per instance, \n",
    "# but again we don’t know the size of the training batch at this point, so the shape is (None) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s create the actual neural network:\n",
    "\n",
    "- The placeholder X will act as the input layer; during the execution phase, it will be replaced with one training batch at a time (note that all the instances in a training batch will be processed simultaneously by the neural network). \n",
    "\n",
    "Now we need to create the two hidden layers and the output layer. \n",
    "\n",
    "- The two hidden layers are almost identical: they differ only by the inputs they are connected to and by the number of neurons they contain. \n",
    "- The output layer is also very similar, but it uses a softmax activation function instead of a ReLU activation function. \n",
    "\n",
    "- So let’s create a neuron_layer() function that we will use to create one layer at a time. It will need parameters to specify the inputs, the number of neurons, the activation function, and the name of the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    \n",
    "    # First we create a name scope using the name of the layer: it will contain all the\n",
    "    #computation nodes for this neuron layer. This is optional, but the graph will look\n",
    "    # much nicer in TensorBoard if its nodes are well organized.\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1]) # we get the number of inputs \n",
    "                                        # by looking up the input matrix’s shape and\n",
    "          # getting the size of the second dimension (the first dimension is for instances).\n",
    "        \n",
    "        # next three lines for creating a W variable that will hold the weights matrix\n",
    "        \n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        \n",
    "        #creates a b variable for biases, initialized to 0 (no symmetry issue in\n",
    "        # this case), with one bias parameter per neuron.\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        # create a subgraph to compute z = X · W + b. This vectorized implementation\n",
    "        # will efficiently compute the weighted sums of the inputs plus the bias term\n",
    "        # for each and every neuron in the layer, for all the instances in the batch in just\n",
    "        # one shot.\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        \n",
    "        # if the activation parameter is set to \"relu\" , the code returns relu(z)\n",
    "        #(i.e., max (0, z)), or else it just returns z .\n",
    "        \n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so now you have a nice function to create a neuron layer. Let’s use it to create\n",
    "the deep neural network! The first hidden layer takes X as its input. The second takes\n",
    "the output of the first hidden layer as its input. And finally, the output layer takes the\n",
    "output of the second hidden layer as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a deep learning neural network, called \"dnn\", with two hidden layers and output layer\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, name=\"hidden1\",\n",
    "                           activation=tf.nn.relu)\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                           activation=tf.nn.relu)\n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "    # note that logits is the output of the neural network \n",
    "    # before going through the softmax activation function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For optimization reasons, we will handle the softmax computation later.\n",
    "As you might expect, TensorFlow comes with many handy functions to create\n",
    "standard neural network layers, so there’s often no need to define your own\n",
    "neuron_layer() function like we just did. \n",
    "\n",
    "For example, TensorFlow’s fully_connected() function creates a fully connected layer, where all the inputs are connected to all the neurons in the layer. It takes care of creating the weights and biases variables, with the proper initialization strategy, and it uses the ReLU activation function by default (we can change this using the activation_fn argument). \n",
    "\n",
    "As we will see in previous Chapter - \"Traing models\", it also supports regularization and normalization parameters.\n",
    "\n",
    "Let’s tweak the preceding code to use the fully_connected() function instead of our neuron_layer() function. Simply import the function and replace the dnn construction\n",
    "section with the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import softmax function, introduced in the lecture-slides.\n",
    "# we will also use cross entropy \n",
    "# ( a  new cost function, not discussed yet in the lecture, simply accept it here)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the neural network model ready to go, we need to define the cost\n",
    "# function that we will use to train it. Just as we did for Softmax Regression, \n",
    "# we will use cross entropy ( a  new cost function, not discussed yet, simply accept it here) . \n",
    "\n",
    "\n",
    "# The cross entropy will penalize models that estimate a low probability for the target class. \n",
    "# TensorFlow provides several functions to compute cross entropy. \n",
    "# Here, we will use sparse_softmax_cross_entropy_with_logits() : \n",
    "# it computes the cross entropy based on the “logits” (i.e., the output of the network \n",
    "# before going through the softmax activation function), \n",
    "# and it expects labels in the form of integers ranging from 0 to the number\n",
    "# of classes minus 1 (in our case, from 0 to 9). This will give us a 1D tensor containing\n",
    "# the cross entropy for each instance. We can then use TensorFlow’s reduce_mean() function, \n",
    "# as above, to compute the mean cross entropy over all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we have the neural network model, we have the cost function, and now we need to\n",
    "# define a GradientDescentOptimizer that will tweak the model parameters to minimize \n",
    "# the cost function. Nothing new; it’s just like before.\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last important step in the construction phase is to specify how to evaluate the\n",
    "model. We will simply use accuracy as our performance measure.\n",
    "\n",
    "- First, for each instance, determine if the neural network’s prediction is correct by checking whether or not the highest logit corresponds to the target class. \n",
    "\n",
    "- For this you can use the in_top_k() function. This returns a 1D tensor full of boolean values, so we need to cast these booleans to floats and then compute the average. This will give us the network’s overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    # take some time for computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as usual, we need to create a node to initialize all variables, and we will also create\n",
    "# a Saver to save our trained model parameters to disk:\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the construction phase. \n",
    "This was fewer than 40 lines of code, but it was pretty intense: we created placeholders for the inputs and the targets, we created a function to build a neuron layer, we used it to create the DNN, we defined the cost function, we created an optimizer, and finally we defined the performance measure. \n",
    "\n",
    "\n",
    "Now on to the execution phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                            y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "    \n",
    "    # it takes some time for 40 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\") # or better, use save_path\n",
    "    X_new_scaled = mnist.test.images[:20]\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted classes:\", y_pred)\n",
    "print(\"Actual classes:   \", mnist.test.labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "nav_menu": {
   "height": "264px",
   "width": "369px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
